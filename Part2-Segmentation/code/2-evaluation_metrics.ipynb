{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970e3902-f82e-40e0-9a81-2254d12aa619",
   "metadata": {},
   "source": [
    "# EMBS-BHI-2025: Robust and Reproducible AI TutorialÂ¶\n",
    "Tutorial instructors: Ernest Namdar and Pascal Tyrrell\n",
    "\n",
    "The dataset used in this tutorial is from BraTS 2025 Pediatrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054449cc",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Pediatric BraTS Segmentation\n",
    "\n",
    "3D segmentation keeps inter-slice context and typically delivers smoother, anatomically consistent predictions, yet 2D networks remain attractive because they are less complex to train, require fewer computational resources, and the manual ground truthing effort per slice is lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deca9e4",
   "metadata": {},
   "source": [
    "## Metric Taxonomy\n",
    "- **Overlap-Based Metrics**\n",
    "- **Distance-Based Metrics**\n",
    "- **Detection / Object-Level Metrics**\n",
    "- **Calibration / Probabilistic Metrics**\n",
    "- **Region / Volume-Based Metrics**\n",
    "- **Advanced / Specialized Metrics**\n",
    "- **Multi-Class Extensions**\n",
    "- **Composite Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a90a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from scipy.ndimage import label, binary_erosion\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "    plt = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad8d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground-truth masks: /home/ernest/Desktop/Projects/EMBS_BHI_2025_Tutorial/Part2-Segmentation/data/GroundTruth\n",
      "Inference masks:    /home/ernest/Desktop/Projects/EMBS_BHI_2025_Tutorial/Part2-Segmentation/data/Inference\n",
      "Results directory:  /home/ernest/Desktop/Projects/EMBS_BHI_2025_Tutorial/Part2-Segmentation/results\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path('.').resolve()\n",
    "if not (PROJECT_ROOT / 'data').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "GROUND_TRUTH_DIR = DATA_DIR / 'GroundTruth'\n",
    "INFERENCE_DIR = DATA_DIR / 'Inference'\n",
    "RESULTS_DIR = PROJECT_ROOT / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Ground-truth masks: {GROUND_TRUTH_DIR}')\n",
    "print(f'Inference masks:    {INFERENCE_DIR}')\n",
    "print(f'Results directory:  {RESULTS_DIR}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bcbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_canonical_mask(path: Path):\n",
    "    image = nib.load(str(path))\n",
    "    canonical = nib.as_closest_canonical(image)\n",
    "    data = canonical.get_fdata()\n",
    "    mask = data > 0\n",
    "    zooms = canonical.header.get_zooms()[:3]\n",
    "    return mask.astype(bool), np.asarray(zooms, dtype=float)\n",
    "\n",
    "def strip_nii_gz(name: str) -> str:\n",
    "    return name[:-7] if name.endswith('.nii.gz') else Path(name).stem\n",
    "\n",
    "def _surface_voxels(mask: np.ndarray):\n",
    "    if not mask.any():\n",
    "        return np.empty((0, mask.ndim), dtype=float)\n",
    "    eroded = binary_erosion(mask, structure=np.ones((3, 3, 3)))\n",
    "    surface = mask ^ eroded\n",
    "    coords = np.argwhere(surface)\n",
    "    return coords\n",
    "\n",
    "def _voxel_to_world(coords: np.ndarray, spacing):\n",
    "    return coords * spacing\n",
    "\n",
    "def _surface_distance_arrays(gt_surface, pred_surface, spacing):\n",
    "    if gt_surface.size == 0 and pred_surface.size == 0:\n",
    "        return np.array([0.0]), np.array([0.0])\n",
    "    if gt_surface.size == 0:\n",
    "        return np.array([np.inf]), np.zeros(pred_surface.shape[0])\n",
    "    if pred_surface.size == 0:\n",
    "        return np.zeros(gt_surface.shape[0]), np.array([np.inf])\n",
    "    gt_world = _voxel_to_world(gt_surface, spacing)\n",
    "    pred_world = _voxel_to_world(pred_surface, spacing)\n",
    "\n",
    "    gt_tree = cKDTree(gt_world)\n",
    "    pred_tree = cKDTree(pred_world)\n",
    "\n",
    "    d_gt_to_pred, _ = pred_tree.query(gt_world)\n",
    "    d_pred_to_gt, _ = gt_tree.query(pred_world)\n",
    "\n",
    "    return d_gt_to_pred, d_pred_to_gt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8e76f",
   "metadata": {},
   "source": [
    "### Overlap-Based Metrics\n",
    "Dice, Jaccard/IoU, and Relative Volume Difference quantify volumetric agreement. They are easy to interpret but insensitive to small boundary errors when volumes are large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afc71a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true, y_pred).sum()\n",
    "    volumes = y_true.sum() + y_pred.sum()\n",
    "    if volumes == 0:\n",
    "        return 1.0\n",
    "    return 2.0 * intersection / volumes\n",
    "\n",
    "def jaccard_index(y_true, y_pred):\n",
    "    intersection = np.logical_and(y_true, y_pred).sum()\n",
    "    union = np.logical_or(y_true, y_pred).sum()\n",
    "    if union == 0:\n",
    "        return 1.0\n",
    "    return intersection / union\n",
    "\n",
    "def relative_volume_difference(y_true, y_pred):\n",
    "    gt_vol = y_true.sum()\n",
    "    pred_vol = y_pred.sum()\n",
    "    if gt_vol == 0:\n",
    "        return np.inf if pred_vol > 0 else 0.0\n",
    "    return (pred_vol - gt_vol) / gt_vol\n",
    "\n",
    "def absolute_volume_difference(y_true, y_pred):\n",
    "    return abs(y_pred.sum() - y_true.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91b7af",
   "metadata": {},
   "source": [
    "### Distance-Based Metrics\n",
    "Hausdorff-style distances respond to worst-case boundary errors; percentile variants trim outliers. Surface Dice and NSD reward boundary agreement within a tolerance but depend on spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f735ded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hausdorff_distance(y_true, y_pred, spacing):\n",
    "    gt_surface = _surface_voxels(y_true)\n",
    "    pred_surface = _surface_voxels(y_pred)\n",
    "    d_gt_to_pred, d_pred_to_gt = _surface_distance_arrays(gt_surface, pred_surface, spacing)\n",
    "    if np.isinf(d_gt_to_pred).any() or np.isinf(d_pred_to_gt).any():\n",
    "        return np.inf\n",
    "    return max(d_gt_to_pred.max(), d_pred_to_gt.max())\n",
    "\n",
    "def percentile_hausdorff(y_true, y_pred, spacing, percentile=95):\n",
    "    gt_surface = _surface_voxels(y_true)\n",
    "    pred_surface = _surface_voxels(y_pred)\n",
    "    d_gt_to_pred, d_pred_to_gt = _surface_distance_arrays(gt_surface, pred_surface, spacing)\n",
    "    distances = np.concatenate([d_gt_to_pred, d_pred_to_gt])\n",
    "    if np.isinf(distances).any():\n",
    "        return np.inf\n",
    "    return np.percentile(distances, percentile)\n",
    "\n",
    "def mean_surface_distance(y_true, y_pred, spacing):\n",
    "    gt_surface = _surface_voxels(y_true)\n",
    "    pred_surface = _surface_voxels(y_pred)\n",
    "    d_gt_to_pred, d_pred_to_gt = _surface_distance_arrays(gt_surface, pred_surface, spacing)\n",
    "    distances = np.concatenate([d_gt_to_pred, d_pred_to_gt])\n",
    "    if np.isinf(distances).any():\n",
    "        return np.inf\n",
    "    return distances.mean()\n",
    "\n",
    "def normalized_surface_dice(y_true, y_pred, spacing, tolerance=1.0):\n",
    "    gt_surface = _surface_voxels(y_true)\n",
    "    pred_surface = _surface_voxels(y_pred)\n",
    "    if gt_surface.size == 0 and pred_surface.size == 0:\n",
    "        return 1.0\n",
    "    d_gt_to_pred, d_pred_to_gt = _surface_distance_arrays(gt_surface, pred_surface, spacing)\n",
    "    gt_within = (d_gt_to_pred <= tolerance).sum()\n",
    "    pred_within = (d_pred_to_gt <= tolerance).sum()\n",
    "    denom = gt_surface.shape[0] + pred_surface.shape[0]\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return (gt_within + pred_within) / denom\n",
    "\n",
    "def surface_dice(y_true, y_pred, spacing, tolerance=1.0):\n",
    "    gt_surface = _surface_voxels(y_true)\n",
    "    pred_surface = _surface_voxels(y_pred)\n",
    "    if gt_surface.size == 0 and pred_surface.size == 0:\n",
    "        return 1.0\n",
    "    d_gt_to_pred, d_pred_to_gt = _surface_distance_arrays(gt_surface, pred_surface, spacing)\n",
    "    sensitivity = (d_gt_to_pred <= tolerance).sum() / max(gt_surface.shape[0], 1)\n",
    "    precision = (d_pred_to_gt <= tolerance).sum() / max(pred_surface.shape[0], 1)\n",
    "    if sensitivity + precision == 0:\n",
    "        return 0.0\n",
    "    return 2 * sensitivity * precision / (sensitivity + precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4ade7",
   "metadata": {},
   "source": [
    "### Detection / Object-Level Metrics\n",
    "Lesion-wise statistics look at connected components and highlight under/over-segmentation patterns that voxelwise scores can miss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48ed11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesionwise_stats(y_true, y_pred):\n",
    "    labeled_true, num_true = label(y_true)\n",
    "    labeled_pred, num_pred = label(y_pred)\n",
    "\n",
    "    if num_true == 0:\n",
    "        recall = 1.0 if num_pred == 0 else 0.0\n",
    "    else:\n",
    "        hits = 0\n",
    "        for idx in range(1, num_true + 1):\n",
    "            component = labeled_true == idx\n",
    "            if np.logical_and(component, y_pred).any():\n",
    "                hits += 1\n",
    "        recall = hits / num_true\n",
    "\n",
    "    if num_pred == 0:\n",
    "        precision = 1.0 if num_true == 0 else 0.0\n",
    "    else:\n",
    "        hits = 0\n",
    "        for idx in range(1, num_pred + 1):\n",
    "            component = labeled_pred == idx\n",
    "            if np.logical_and(component, y_true).any():\n",
    "                hits += 1\n",
    "        precision = hits / num_pred\n",
    "\n",
    "    return recall, precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47487b",
   "metadata": {},
   "source": [
    "### Region / Volume-Based Metrics\n",
    "Absolute and relative volume differences are critical for downstream treatment planning, but they ignore shape fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb98440",
   "metadata": {},
   "source": [
    "### Advanced / Specialized Metrics\n",
    "Mean Surface Distance (MSD) complements Hausdorff scores by averaging surface discrepancies; it is less sensitive to single outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f5382d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred, spacing):\n",
    "    metrics = {}\n",
    "    metrics['dice'] = dice_coefficient(y_true, y_pred)\n",
    "    metrics['jaccard'] = jaccard_index(y_true, y_pred)\n",
    "    metrics['rvd'] = relative_volume_difference(y_true, y_pred)\n",
    "    metrics['avd_voxels'] = absolute_volume_difference(y_true, y_pred)\n",
    "\n",
    "    metrics['hausdorff'] = hausdorff_distance(y_true, y_pred, spacing)\n",
    "    metrics['hausdorff95'] = percentile_hausdorff(y_true, y_pred, spacing, percentile=95)\n",
    "    metrics['nsd'] = normalized_surface_dice(y_true, y_pred, spacing, tolerance=1.0)\n",
    "    metrics['surface_dice'] = surface_dice(y_true, y_pred, spacing, tolerance=1.0)\n",
    "    metrics['msd'] = mean_surface_distance(y_true, y_pred, spacing)\n",
    "\n",
    "    recall, precision = lesionwise_stats(y_true, y_pred)\n",
    "    metrics['lesion_recall'] = recall\n",
    "    metrics['lesion_precision'] = precision\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5fd10",
   "metadata": {},
   "source": [
    "### Batch Evaluation Loop\n",
    "The loop below iterates over every case, reports progress every 10 subjects, and saves the aggregated metrics for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffe0fa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cases to process: 261\n"
     ]
    }
   ],
   "source": [
    "def collect_case_ids():\n",
    "    gt_cases = {strip_nii_gz(p.name) for p in GROUND_TRUTH_DIR.glob('*.nii.gz')}\n",
    "    pred_cases = {strip_nii_gz(p.name) for p in INFERENCE_DIR.glob('*.nii.gz')}\n",
    "    common = sorted(gt_cases & pred_cases)\n",
    "    missing_preds = sorted(gt_cases - pred_cases)\n",
    "    missing_gts = sorted(pred_cases - gt_cases)\n",
    "    if missing_preds:\n",
    "        print(f'Missing inference for {len(missing_preds)} cases (first 5): {missing_preds[:5]}')\n",
    "    if missing_gts:\n",
    "        print(f'Missing ground truth for {len(missing_gts)} cases (first 5): {missing_gts[:5]}')\n",
    "    return common\n",
    "\n",
    "case_ids = collect_case_ids()\n",
    "print(f'Total cases to process: {len(case_ids)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aae34b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 / 261 cases...\n",
      "Processed 20 / 261 cases...\n",
      "Processed 30 / 261 cases...\n",
      "Processed 40 / 261 cases...\n",
      "Processed 50 / 261 cases...\n",
      "Processed 60 / 261 cases...\n",
      "Processed 70 / 261 cases...\n",
      "Processed 80 / 261 cases...\n",
      "Processed 90 / 261 cases...\n",
      "Processed 100 / 261 cases...\n",
      "Processed 110 / 261 cases...\n",
      "Processed 120 / 261 cases...\n",
      "Processed 130 / 261 cases...\n",
      "Processed 140 / 261 cases...\n",
      "Processed 150 / 261 cases...\n",
      "Processed 160 / 261 cases...\n",
      "Processed 170 / 261 cases...\n",
      "Processed 180 / 261 cases...\n",
      "Processed 190 / 261 cases...\n",
      "Processed 200 / 261 cases...\n",
      "Processed 210 / 261 cases...\n",
      "Processed 220 / 261 cases...\n",
      "Processed 230 / 261 cases...\n",
      "Processed 240 / 261 cases...\n",
      "Processed 250 / 261 cases...\n",
      "Processed 260 / 261 cases...\n",
      "Processed 261 / 261 cases...\n",
      "Saved metrics to results/evaluation_metrics.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dice</th>\n",
       "      <th>jaccard</th>\n",
       "      <th>rvd</th>\n",
       "      <th>avd_voxels</th>\n",
       "      <th>hausdorff</th>\n",
       "      <th>hausdorff95</th>\n",
       "      <th>nsd</th>\n",
       "      <th>surface_dice</th>\n",
       "      <th>msd</th>\n",
       "      <th>lesion_recall</th>\n",
       "      <th>lesion_precision</th>\n",
       "      <th>case_id</th>\n",
       "      <th>voxel_spacing_x</th>\n",
       "      <th>voxel_spacing_y</th>\n",
       "      <th>voxel_spacing_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939661</td>\n",
       "      <td>0.886189</td>\n",
       "      <td>-0.001710</td>\n",
       "      <td>220</td>\n",
       "      <td>10.440307</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.918363</td>\n",
       "      <td>0.919463</td>\n",
       "      <td>0.432630</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BraTS-PED-00001-000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.974891</td>\n",
       "      <td>0.951011</td>\n",
       "      <td>-0.015203</td>\n",
       "      <td>266</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995115</td>\n",
       "      <td>0.995155</td>\n",
       "      <td>0.159473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BraTS-PED-00002-000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943192</td>\n",
       "      <td>0.892491</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>189</td>\n",
       "      <td>84.693565</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.931390</td>\n",
       "      <td>0.933062</td>\n",
       "      <td>0.480986</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BraTS-PED-00003-000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.948302</td>\n",
       "      <td>0.901687</td>\n",
       "      <td>0.005383</td>\n",
       "      <td>844</td>\n",
       "      <td>40.422766</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.819089</td>\n",
       "      <td>0.822027</td>\n",
       "      <td>0.715332</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BraTS-PED-00004-000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.945409</td>\n",
       "      <td>0.896470</td>\n",
       "      <td>-0.002320</td>\n",
       "      <td>40</td>\n",
       "      <td>5.830952</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>0.920215</td>\n",
       "      <td>0.921760</td>\n",
       "      <td>0.416009</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BraTS-PED-00005-000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dice   jaccard       rvd  avd_voxels  hausdorff  hausdorff95       nsd  \\\n",
       "0  0.939661  0.886189 -0.001710         220  10.440307     1.414214  0.918363   \n",
       "1  0.974891  0.951011 -0.015203         266   4.000000     1.000000  0.995115   \n",
       "2  0.943192  0.892491  0.011384         189  84.693565     1.414214  0.931390   \n",
       "3  0.948302  0.901687  0.005383         844  40.422766     2.449490  0.819089   \n",
       "4  0.945409  0.896470 -0.002320          40   5.830952     1.414214  0.920215   \n",
       "\n",
       "   surface_dice       msd  lesion_recall  lesion_precision  \\\n",
       "0      0.919463  0.432630       0.111111               1.0   \n",
       "1      0.995155  0.159473       0.500000               1.0   \n",
       "2      0.933062  0.480986       0.200000               1.0   \n",
       "3      0.822027  0.715332       0.100000               1.0   \n",
       "4      0.921760  0.416009       0.250000               1.0   \n",
       "\n",
       "               case_id  voxel_spacing_x  voxel_spacing_y  voxel_spacing_z  \n",
       "0  BraTS-PED-00001-000              1.0              1.0              1.0  \n",
       "1  BraTS-PED-00002-000              1.0              1.0              1.0  \n",
       "2  BraTS-PED-00003-000              1.0              1.0              1.0  \n",
       "3  BraTS-PED-00004-000              1.0              1.0              1.0  \n",
       "4  BraTS-PED-00005-000              1.0              1.0              1.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for idx, case_id in enumerate(case_ids, start=1):\n",
    "    gt_mask, spacing = load_canonical_mask(GROUND_TRUTH_DIR / f'{case_id}.nii.gz')\n",
    "    pred_mask, _ = load_canonical_mask(INFERENCE_DIR / f'{case_id}.nii.gz')\n",
    "\n",
    "    metrics = compute_metrics(gt_mask, pred_mask, spacing)\n",
    "    metrics['case_id'] = case_id\n",
    "    metrics['voxel_spacing_x'] = spacing[0]\n",
    "    metrics['voxel_spacing_y'] = spacing[1]\n",
    "    metrics['voxel_spacing_z'] = spacing[2]\n",
    "    results.append(metrics)\n",
    "\n",
    "    if idx % 10 == 0 or idx == len(case_ids):\n",
    "        print(f'Processed {idx} / {len(case_ids)} cases...')\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "output_csv = RESULTS_DIR / 'evaluation_metrics.csv'\n",
    "metrics_df.to_csv(output_csv, index=False)\n",
    "print(f'Saved metrics to {output_csv.relative_to(PROJECT_ROOT)}')\n",
    "metrics_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed761fe9",
   "metadata": {},
   "source": [
    "Calibration, multi-class, and composite scoring layers can be added on top of these voxel-level metrics once probabilistic outputs or subregion annotations are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ed1dcf",
   "metadata": {},
   "source": [
    "BraTS remains a multi-subregion benchmark; although we focus on whole-tumor masks here, the same evaluation scaffolding extends naturally to per-label assessments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
